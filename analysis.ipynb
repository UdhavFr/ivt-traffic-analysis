{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5224ce7c-4f8b-4b3a-86b0-a1ba5b2b4280",
   "metadata": {},
   "source": [
    "Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7ffccb-c1e6-4675-8c85-daa079ce8184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "output_dir = Path(\"outputs\")\n",
    "figures_dir = Path(\"figures\")\n",
    "\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "if not figures_dir.exists():\n",
    "    figures_dir.mkdir()\n",
    "\n",
    "print(\"Environment setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e75ad-df66-44f3-96f0-fdc13c72aaa4",
   "metadata": {},
   "source": [
    "Loading Data and extracting daily/hourly blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32f17f7-74bf-472b-bcb7-a02949bb5288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily rows: 534 | Hourly rows: 504\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the workbook\n",
    "excel_path = Path(\"data/Data-Analytics-Assignment.xlsx\")\n",
    "workbook = pd.ExcelFile(excel_path)\n",
    "\n",
    "\n",
    "def extract_block(raw_data, block_label):\n",
    "    \"\"\"Extracts the specified data block (Daily/Hourly) from a given worksheet.\"\"\"\n",
    "    match_row = None\n",
    "    for idx in range(raw_data.shape[0]):\n",
    "        row_values = raw_data.iloc[idx].astype(str).str.strip().str.lower()\n",
    "        if row_values.eq(block_label.lower()).any():\n",
    "            match_row = idx\n",
    "            break\n",
    "\n",
    "    if match_row is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    header_row = match_row + 1\n",
    "    if header_row >= raw_data.shape[0]:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Copy everything below the header\n",
    "    df = raw_data.iloc[header_row:].copy()\n",
    "    df.columns = raw_data.iloc[header_row].astype(str).str.strip()\n",
    "    df = df.iloc[1:]  # skip header row itself\n",
    "\n",
    "    # Standardize the date column name if needed\n",
    "    if \"Date Or Hour\" in df.columns and \"Date\" not in df.columns:\n",
    "        df.rename(columns={\"Date Or Hour\": \"Date\"}, inplace=True)\n",
    "\n",
    "    if \"Date\" not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df[\"Date_parsed\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df[df[\"Date_parsed\"].notna()].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "daily_dataframes = []\n",
    "hourly_dataframes = []\n",
    "\n",
    "for sheet_name in workbook.sheet_names:\n",
    "    sheet_df = workbook.parse(sheet_name, header=None)\n",
    "\n",
    "    daily_df = extract_block(sheet_df, \"Daily Data\")\n",
    "    hourly_df = extract_block(sheet_df, \"Hourly Data\")\n",
    "\n",
    "    for dframe in (daily_df, hourly_df):\n",
    "        if dframe.empty:\n",
    "            continue\n",
    "        dframe.columns = [str(c).strip() for c in dframe.columns]\n",
    "        dframe[\"app_id\"] = sheet_name\n",
    "        dframe[\"status\"] = \"Valid\" if \"Valid\" in sheet_name else \"Invalid\"\n",
    "\n",
    "    if not daily_df.empty:\n",
    "        daily_dataframes.append(daily_df)\n",
    "    if not hourly_df.empty:\n",
    "        hourly_dataframes.append(hourly_df)\n",
    "\n",
    "# Combine results\n",
    "daily_master = pd.concat(daily_dataframes, ignore_index=True) if daily_dataframes else pd.DataFrame()\n",
    "hourly_master = pd.concat(hourly_dataframes, ignore_index=True) if hourly_dataframes else pd.DataFrame()\n",
    "\n",
    "# Clean up numeric columns\n",
    "num_cols = [\n",
    "    \"unique_idfas\", \"unique_ips\", \"unique_uas\", \"total_requests\",\n",
    "    \"requests_per_idfa\", \"impressions\", \"impressions_per_idfa\",\n",
    "    \"idfa_ip_ratio\", \"idfa_ua_ratio\", \"IVT\"\n",
    "]\n",
    "\n",
    "for col in num_cols:\n",
    "    if col in daily_master.columns:\n",
    "        daily_master[col] = pd.to_numeric(daily_master[col], errors=\"coerce\")\n",
    "    if col in hourly_master.columns:\n",
    "        hourly_master[col] = pd.to_numeric(hourly_master[col], errors=\"coerce\")\n",
    "\n",
    "print(f\"Daily rows: {len(daily_master)} | Hourly rows: {len(hourly_master)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477cf805-7d19-47b3-a0ee-fe6609540e17",
   "metadata": {},
   "source": [
    "Cleaning up and filtering to true Daily Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df0fee7-a55f-4dcf-a0df-3a97526dbc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned datasets: 54 daily rows, 504 hourly rows.\n",
      "Apps included: ['Valid 1', 'Valid 2', 'Valid 3', 'Invalid 1', 'Invalid 2', 'Invalid 3']\n"
     ]
    }
   ],
   "source": [
    "# Remove stray NaN or unnamed columns\n",
    "for frame in (daily_master, hourly_master):\n",
    "    drop_cols = [\n",
    "        c for c in frame.columns \n",
    "        if str(c).strip().lower() in ('nan', 'unnamed: 0', 'unnamed:0', '')\n",
    "    ]\n",
    "    if drop_cols:\n",
    "        frame.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Some daily sheets have mixed hourly data â€” filter for true daily (00:00)\n",
    "midnight_time = pd.to_datetime(\"00:00:00\").time()\n",
    "daily_clean = daily_master[daily_master[\"Date_parsed\"].dt.time == midnight_time].copy()\n",
    "hourly_clean = hourly_master.copy()\n",
    "\n",
    "# Save cleaned data\n",
    "output_dir = Path(\"outputs\")\n",
    "daily_path = output_dir / \"daily_master_clean.csv\"\n",
    "hourly_path = output_dir / \"hourly_master_clean.csv\"\n",
    "\n",
    "daily_clean.to_csv(daily_path, index=False)\n",
    "hourly_clean.to_csv(hourly_path, index=False)\n",
    "\n",
    "print(f\"Saved cleaned datasets: {len(daily_clean)} daily rows, {len(hourly_clean)} hourly rows.\")\n",
    "if \"app_id\" in daily_clean.columns:\n",
    "    apps = daily_clean[\"app_id\"].unique().tolist()\n",
    "    print(f\"Apps included: {apps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3033d202-3879-4ba2-9506-99f5660a1fe7",
   "metadata": {},
   "source": [
    "EDA : Summary Stats by App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2be1ab9-98c9-41fd-8693-29d86f0c4ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Stats by App\n",
      "   app_id  status  unique_idfas_median  unique_idfas_mean  unique_idfas_std  unique_ips_median  unique_ips_mean  unique_ips_std  unique_uas_median  unique_uas_mean  unique_uas_std  idfa_ua_ratio_median  idfa_ua_ratio_mean  idfa_ua_ratio_std  idfa_ip_ratio_median  idfa_ip_ratio_mean  idfa_ip_ratio_std  requests_per_idfa_median  requests_per_idfa_mean  requests_per_idfa_std  IVT_median  IVT_mean  IVT_std\n",
      "Invalid 1 Invalid              55073.0      120811.666667     128745.976541            55062.0    120747.333333   128653.852368              222.0       219.000000       42.311346            248.076577          482.229666         465.002180              1.000196            1.000275           0.000288                  1.093730                1.108404               0.082892    0.994737  0.751958 0.431354\n",
      "Invalid 2 Invalid              28822.0       63570.222222      67748.358544            28819.0     63527.777778    67688.343826              834.0       864.111111      279.687971             34.558753           59.240115          52.873389              1.000104            1.000329           0.000365                  1.060128                1.080202               0.063295    0.999106  0.917086 0.241165\n",
      "Invalid 3 Invalid              27020.0       92665.444444     100168.209258            27015.0     92174.000000    99503.608002              199.0       198.111111       49.921049            135.778894          391.945534         378.281444              1.000774            1.002647           0.002838                  1.088305                1.166281               0.122350    0.785315  0.532108 0.503754\n",
      "  Valid 1   Valid              93345.0      186205.111111     195961.736831            93340.0    186107.333333   195821.142547               21.0        22.222222        2.488864           4242.954545         7684.114554        7559.495823              1.000102            1.000261           0.000289                  1.063808                1.098377               0.080433    0.005786  0.004917 0.001474\n",
      "  Valid 2   Valid              48280.0      104969.333333     111453.910909            48234.0    104461.666667   110749.673129              174.0       189.222222       57.768455            267.068966          450.257704         397.767739              1.000954            1.002475           0.002580                  1.086535                1.105253               0.065411    0.058381  0.061535 0.016276\n",
      "  Valid 3   Valid              22684.0      122754.888889     134326.146984            22648.0    120390.111111   131287.749243               22.0        22.000000        0.000000           1031.090909         5579.767677        6105.733954              1.001640            1.009497           0.010473                  1.071769                1.132782               0.101215    0.004753  0.003914 0.003041\n",
      "\n",
      "Summary saved to: outputs\\summary_by_app.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics grouped by app and status\n",
    "summary = (\n",
    "    daily_clean.groupby([\"app_id\", \"status\"])[\n",
    "        [\n",
    "            \"unique_idfas\", \"unique_ips\", \"unique_uas\",\n",
    "            \"idfa_ua_ratio\", \"idfa_ip_ratio\",\n",
    "            \"requests_per_idfa\", \"IVT\"\n",
    "        ]\n",
    "    ]\n",
    "    .agg([\"median\", \"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten multi-level column headers for easier CSV export\n",
    "summary.columns = [\"_\".join(col).strip(\"_\") for col in summary.columns.to_flat_index()]\n",
    "\n",
    "print(\"\\nSummary Stats by App\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "output_file = Path(\"outputs\") / \"summary_by_app.csv\"\n",
    "summary.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nSummary saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db133e07-edfb-41f7-a967-ce0faef436db",
   "metadata": {},
   "source": [
    "Finding first IVT spike per app (hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb782fcf-bd8c-4195-b4eb-e5c78658a789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First High IVT (>0.5) Timestamp\n",
      "   app_id      first_high_IVT\n",
      "Invalid 1 2025-09-12 05:00:00\n",
      "Invalid 2 2025-09-11 21:00:00\n",
      "Invalid 3 2025-09-13 05:00:00\n",
      "\n",
      "Saved: outputs\\first_high_IVT_per_app.csv\n",
      "\n",
      "Flag Classification\n",
      "   app_id      first_high_IVT     flag_type\n",
      "  Valid 1                 NaT Never flagged\n",
      "  Valid 2                 NaT Never flagged\n",
      "  Valid 3                 NaT Never flagged\n",
      "Invalid 1 2025-09-12 05:00:00       Delayed\n",
      "Invalid 2 2025-09-11 21:00:00         Early\n",
      "Invalid 3 2025-09-13 05:00:00       Delayed\n"
     ]
    }
   ],
   "source": [
    "# Sort hourly data by app and timestamp\n",
    "hourly_sorted = hourly_clean.sort_values([\"app_id\", \"Date_parsed\"])\n",
    "\n",
    "# Identify first occurrence where IVT exceeds 0.5\n",
    "first_ivt_high = (\n",
    "    hourly_sorted[hourly_sorted[\"IVT\"] > 0.5]\n",
    "    .groupby(\"app_id\", as_index=False)[\"Date_parsed\"]\n",
    "    .min()\n",
    "    .rename(columns={\"Date_parsed\": \"first_high_IVT\"})\n",
    ")\n",
    "\n",
    "print(\"\\nFirst High IVT (>0.5) Timestamp\")\n",
    "print(first_ivt_high.to_string(index=False))\n",
    "\n",
    "# Save the results\n",
    "ivt_output_path = Path(\"outputs\") / \"first_high_IVT_per_app.csv\"\n",
    "first_ivt_high.to_csv(ivt_output_path, index=False)\n",
    "print(f\"\\nSaved: {ivt_output_path}\")\n",
    "\n",
    "# Check which apps never crossed the IVT threshold\n",
    "all_apps = pd.DataFrame({\"app_id\": daily_clean[\"app_id\"].unique()})\n",
    "ivt_flags = all_apps.merge(first_ivt_high, on=\"app_id\", how=\"left\")\n",
    "\n",
    "# Classify apps based on their first IVT breach timing\n",
    "def classify_flag(ts):\n",
    "    if pd.isna(ts):\n",
    "        return \"Never flagged\"\n",
    "    ts_date = pd.to_datetime(ts)\n",
    "    cutoff = pd.to_datetime(\"2025-09-12\")\n",
    "    if ts_date <= cutoff:\n",
    "        return \"Early\"\n",
    "    return \"Delayed\"\n",
    "\n",
    "ivt_flags[\"flag_type\"] = ivt_flags[\"first_high_IVT\"].apply(classify_flag)\n",
    "\n",
    "print(\"\\nFlag Classification\")\n",
    "print(ivt_flags[[\"app_id\", \"first_high_IVT\", \"flag_type\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce479a2d-c886-49f3-a560-49bda20f98c1",
   "metadata": {},
   "source": [
    "Before and after comparison for Invalid apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99638e3c-0f4e-41f3-bfd6-f784c5957e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before / After IVT Spike Comparison\n",
      "\n",
      "App: Invalid 1  |  Spike at: 2025-09-12 05:00:00\n",
      "  Days before: 3  |  After: 6\n",
      "  idfa_ua_ratio: 622.60 â†’ 412.05 (0.66Ã—)\n",
      "  requests_per_idfa: 1.12 â†’ 1.10 (0.99Ã—)\n",
      "  IVT: 0.261 â†’ 0.998\n",
      "\n",
      "App: Invalid 2  |  Spike at: 2025-09-11 21:00:00\n",
      "  Days before: 1  |  After: 8\n",
      "  idfa_ua_ratio: 33.66 â†’ 62.44 (1.86Ã—)\n",
      "  requests_per_idfa: 1.04 â†’ 1.09 (1.05Ã—)\n",
      "  IVT: 0.274 â†’ 0.997\n",
      "\n",
      "App: Invalid 3  |  Spike at: 2025-09-13 05:00:00\n",
      "  Days before: 5  |  After: 4\n",
      "  idfa_ua_ratio: 485.11 â†’ 275.50 (0.57Ã—)\n",
      "  requests_per_idfa: 1.21 â†’ 1.12 (0.93Ã—)\n",
      "  IVT: 0.162 â†’ 0.995\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Iterate through each appâ€™s first high IVT record\n",
    "for _, row in ivt_flags.iterrows():\n",
    "    app_id = row[\"app_id\"]\n",
    "    spike_time = row[\"first_high_IVT\"]\n",
    "\n",
    "    if pd.isna(spike_time):  # skip apps that never crossed the threshold\n",
    "        continue\n",
    "\n",
    "    app_df = daily_clean[daily_clean[\"app_id\"] == app_id].sort_values(\"Date_parsed\")\n",
    "    before_spike = app_df[app_df[\"Date_parsed\"] < spike_time]\n",
    "    after_spike = app_df[app_df[\"Date_parsed\"] >= spike_time]\n",
    "\n",
    "    def calc_stats(sub):\n",
    "        return {\n",
    "            \"rows\": len(sub),\n",
    "            \"mean_unique_idfas\": sub[\"unique_idfas\"].mean(),\n",
    "            \"mean_unique_uas\": sub[\"unique_uas\"].mean(),\n",
    "            \"mean_requests_per_idfa\": sub[\"requests_per_idfa\"].mean(),\n",
    "            \"mean_idfa_ua_ratio\": sub[\"idfa_ua_ratio\"].mean(),\n",
    "            \"mean_idfa_ip_ratio\": sub[\"idfa_ip_ratio\"].mean(),\n",
    "            \"mean_IVT\": sub[\"IVT\"].mean()\n",
    "        }\n",
    "\n",
    "    results.append({\n",
    "        \"app_id\": app_id,\n",
    "        \"first_high_IVT\": spike_time,\n",
    "        \"before\": calc_stats(before_spike),\n",
    "        \"after\": calc_stats(after_spike)\n",
    "    })\n",
    "\n",
    "print(\"\\nBefore / After IVT Spike Comparison\")\n",
    "for r in results:\n",
    "    print(f\"\\nApp: {r['app_id']}  |  Spike at: {r['first_high_IVT']}\")\n",
    "    print(f\"  Days before: {r['before']['rows']}  |  After: {r['after']['rows']}\")\n",
    "    print(f\"  idfa_ua_ratio: {r['before']['mean_idfa_ua_ratio']:.2f} â†’ {r['after']['mean_idfa_ua_ratio']:.2f} \"\n",
    "          f\"({r['after']['mean_idfa_ua_ratio']/r['before']['mean_idfa_ua_ratio']:.2f}Ã—)\")\n",
    "    print(f\"  requests_per_idfa: {r['before']['mean_requests_per_idfa']:.2f} â†’ {r['after']['mean_requests_per_idfa']:.2f} \"\n",
    "          f\"({r['after']['mean_requests_per_idfa']/r['before']['mean_requests_per_idfa']:.2f}Ã—)\")\n",
    "    print(f\"  IVT: {r['before']['mean_IVT']:.3f} â†’ {r['after']['mean_IVT']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e1d10-a0ac-4f8d-9120-bd8aec59c3c0",
   "metadata": {},
   "source": [
    "Visualizing IVT over time per app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9236a1-93f0-4f6e-8989-79b9086dbf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved IVT timeline plots to figures/\n"
     ]
    }
   ],
   "source": [
    "fig_dir = Path('figures')\n",
    "\n",
    "for app in hourly_clean['app_id'].unique():\n",
    "    df_app = hourly_clean[hourly_clean['app_id'] == app].sort_values('Date_parsed')\n",
    "    \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(df_app['Date_parsed'], df_app['IVT'], marker='o', markersize=3, linestyle='-', linewidth=1)\n",
    "    plt.axhline(0.5, color='red', linestyle='--', linewidth=1, label='IVT threshold (0.5)')\n",
    "    plt.title(f'IVT over Time â€” {app}', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Date/Hour')\n",
    "    plt.ylabel('IVT')\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_dir / f'IVT_timeline_{app.replace(\" \", \"_\")}.png', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Saved IVT timeline plots to figures/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41818f28-1acb-41bb-b43e-7a369b4361dd",
   "metadata": {},
   "source": [
    "Comparing Valid vs Invalid - key distribution ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94dca8ec-f94a-4eff-910e-9802d94d6547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved before_after_deltas.csv and app_flag_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dm = pd.read_csv('outputs/daily_master_clean.csv', parse_dates=['Date_parsed'])\n",
    "fh = pd.read_csv('outputs/first_high_IVT_per_app.csv', parse_dates=['first_high_IVT'])\n",
    "sb = pd.read_csv('outputs/summary_by_app.csv')\n",
    "\n",
    "metrics = ['IVT','idfa_ua_ratio','idfa_ip_ratio','requests_per_idfa','unique_idfas','unique_uas']\n",
    "rows = []\n",
    "for _, r in fh.iterrows():\n",
    "    app, t = r['app_id'], r['first_high_IVT']\n",
    "    dfa = dm[dm['app_id']==app].sort_values('Date_parsed')\n",
    "    before, after = dfa[dfa['Date_parsed']<t], dfa[dfa['Date_parsed']>=t]\n",
    "    if before.empty or after.empty: \n",
    "        continue\n",
    "    for m in metrics:\n",
    "        b, a = before[m].mean(), after[m].mean()\n",
    "        rows.append({'app_id': app, 'first_high_IVT': t, 'metric': m,\n",
    "                     'before_mean': b, 'after_mean': a,\n",
    "                     'delta_abs': a-b, 'delta_ratio': (a/(b if b!=0 else 1))})\n",
    "\n",
    "deltas = pd.DataFrame(rows)\n",
    "deltas.to_csv('outputs/before_after_deltas.csv', index=False)\n",
    "\n",
    "apps = sb[['app_id','status']].drop_duplicates()\n",
    "flags = apps.merge(fh, on='app_id', how='left')\n",
    "flags['flag_type'] = flags['first_high_IVT'].apply(\n",
    "    lambda x: 'Never flagged' if pd.isna(x) else 'Early' if pd.to_datetime(x) <= pd.to_datetime('2025-09-12') else 'Delayed'\n",
    ")\n",
    "flags.to_csv('outputs/app_flag_summary.csv', index=False)\n",
    "print('Saved before_after_deltas.csv and app_flag_summary.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e53e059-659c-46d6-a817-443594867432",
   "metadata": {},
   "source": [
    "Export Deltas + Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aff8c2de-de9e-47ee-935b-e7293cb90eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved before_after_deltas.csv and app_flag_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dm = pd.read_csv('outputs/daily_master_clean.csv', parse_dates=['Date_parsed'])\n",
    "fh = pd.read_csv('outputs/first_high_IVT_per_app.csv', parse_dates=['first_high_IVT'])\n",
    "sb = pd.read_csv('outputs/summary_by_app.csv')\n",
    "\n",
    "metrics = ['IVT','idfa_ua_ratio','idfa_ip_ratio','requests_per_idfa','unique_idfas','unique_uas']\n",
    "rows = []\n",
    "for _, r in fh.iterrows():\n",
    "    app, t = r['app_id'], r['first_high_IVT']\n",
    "    dfa = dm[dm['app_id']==app].sort_values('Date_parsed')\n",
    "    before, after = dfa[dfa['Date_parsed']<t], dfa[dfa['Date_parsed']>=t]\n",
    "    if before.empty or after.empty: \n",
    "        continue\n",
    "    for m in metrics:\n",
    "        b, a = before[m].mean(), after[m].mean()\n",
    "        rows.append({'app_id': app, 'first_high_IVT': t, 'metric': m,\n",
    "                     'before_mean': b, 'after_mean': a,\n",
    "                     'delta_abs': a-b, 'delta_ratio': (a/(b if b!=0 else 1))})\n",
    "\n",
    "deltas = pd.DataFrame(rows)\n",
    "deltas.to_csv('outputs/before_after_deltas.csv', index=False)\n",
    "\n",
    "apps = sb[['app_id','status']].drop_duplicates()\n",
    "flags = apps.merge(fh, on='app_id', how='left')\n",
    "flags['flag_type'] = flags['first_high_IVT'].apply(\n",
    "    lambda x: 'Never flagged' if pd.isna(x) else 'Early' if pd.to_datetime(x) <= pd.to_datetime('2025-09-12') else 'Delayed'\n",
    ")\n",
    "flags.to_csv('outputs/app_flag_summary.csv', index=False)\n",
    "print('Saved before_after_deltas.csv and app_flag_summary.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
